# Streamlit Dashboard Build and Deploy Pipeline
#
# This pipeline builds the Streamlit dashboard Docker image and deploys it to the server.
# It uses Docker socket mounting to build directly on the server (no SSH needed).
#
# Build Process:
# 1. Build Docker image from web_dashboard/Dockerfile
# 2. Tag image with commit SHA for version tracking
# 3. Stop and remove existing container (if running)
# 4. Deploy new container with environment variables from Woodpecker secrets
#
# Available Secrets (configure in Woodpecker - use lowercase names):
# - supabase_url: Your Supabase project URL
# - supabase_publishable_key: For user authentication
# - supabase_secret_key: For admin scripts and debug operations
# - app_domain: Your application domain (e.g., "ai-trading.drifting.space")
#               Used for magic links, password resets, and cookie domain settings
# - research_database_url: (optional) Postgres connection string for research articles
#                          Format: postgresql://user:password@host:port/database
#                          For Docker container connecting to host Postgres: use host.docker.internal
#                          Example: postgresql://postgres:password@host.docker.internal:5432/trading_db
# - fmp_api_key: (optional) Financial Modeling Prep API key for congress trading module
#                Get from: https://site.financialmodelingprep.com/developer/docs/
#                Required for congress_trades scheduled job
# - webai_cookies_json: (optional) JSON string of cookies for WebAI Pro web-based AI
#                       Format: {"__Secure-1PSID":"value","__Secure-1PSIDTS":"value"}
#                       Extract using: python web_dashboard/extract_ai_cookies.py --browser manual
#                       Required for WebAI Pro model in AI Assistant
#                       Note: Used only for initial cookie setup. A cookie refresher sidecar container
#                       automatically refreshes cookies and writes them to /shared/cookies/webai_cookies.json
#                       The main app reads cookies from the shared volume, not from this environment variable
# - ai_service_web_url: (optional) Web interface URL for AI service (obfuscated)
#                       Only needed if default fallback URL doesn't work
#                       Set via environment variable when generating ai_service.keys.json
# - ollama_base_url: (optional) Ollama API URL (default: http://host.docker.internal:11434)
# - ollama_model: (optional) Default AI model (default: mistral-nemo:12b)
# - ollama_enabled: (optional) Enable AI assistant (default: true)
#
# Note: Secrets are loaded using from_secret in the environment section
# Note: Repository must be marked as "Trusted" in Woodpecker settings to use volumes

steps:
  build-and-deploy:
    image: docker:24
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /home/lance/ai-trading-www:/deploy_target
    environment:
      TZ: America/Vancouver
      SUPABASE_URL:
        from_secret: supabase_url
      SUPABASE_PUBLISHABLE_KEY:
        from_secret: supabase_publishable_key
      SUPABASE_SECRET_KEY:
        from_secret: supabase_secret_key
      APP_DOMAIN:
        from_secret: app_domain
      RESEARCH_DATABASE_URL:
        from_secret: research_database_url
      FMP_API_KEY:
        from_secret: fmp_api_key
      WEBAI_COOKIES_JSON:
        from_secret: webai_cookies_json
      AI_SERVICE_WEB_URL:
        from_secret: ai_service_web_url
    commands:
      # Set timezone (Pacific Time - handles PST/PDT automatically)
      - ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
      
      # Format build timestamp in UTC (will be converted to user's timezone in Python)
      # CI_PIPELINE_STARTED is Unix timestamp, format in UTC
      - |
        if [ -n "$CI_PIPELINE_STARTED" ]; then
          export BUILD_TIMESTAMP=$(date -u -d "@$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u -r "$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u "+%Y-%m-%d %H:%M UTC")
        else
          export BUILD_TIMESTAMP=$(date -u "+%Y-%m-%d %H:%M UTC")
        fi
      
      # Build Docker image from web_dashboard/Dockerfile
      - docker build -f web_dashboard/Dockerfile -t trading-dashboard:latest .
      - docker tag trading-dashboard:latest trading-dashboard:${CI_COMMIT_SHA}
      
      # Stop and remove existing container if it exists
      - docker stop trading-dashboard || true
      - docker rm trading-dashboard || true
      
      # Create logs directory on host with proper permissions
      - mkdir -p /home/lance/trading-dashboard-logs && chmod 777 /home/lance/trading-dashboard-logs
      
      # Verify secrets are set
      - |
        if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_PUBLISHABLE_KEY" ] || [ -z "$SUPABASE_SECRET_KEY" ]; then
          echo "❌ ERROR: Missing required secrets"
          echo "Make sure supabase_url, supabase_publishable_key, and supabase_secret_key are set in Woodpecker"
          exit 1
        fi
      
      # Deploy new container (single line to avoid parsing issues)
      # Added --add-host to allow connecting to Ollama on the host machine via host.docker.internal (Linux support)
      # NOTE: On Windows/Mac, host.docker.internal works natively. On Linux, you MUST add "--add-host=host.docker.internal:host-gateway"
      # Ollama settings can be overridden via Woodpecker secrets or environment variables
      # RESEARCH_DATABASE_URL is optional - only set if research articles storage is needed
      # FMP_API_KEY is optional - only set if congress trading module is needed
      - |
        # Build base docker run command
        DOCKER_CMD="docker run -d --name trading-dashboard --restart unless-stopped -p 8501:8501 --add-host=host.docker.internal:host-gateway"
        # Mount application logs directory (host logs -> container logs)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/trading-dashboard-logs:/app/web_dashboard/logs"
        # Mount Ollama logs directory (host logs -> container logs/server)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/ollama-logs:/app/web_dashboard/logs/server"
        # Mount shared cookies directory (for cookie refresher sidecar)
        DOCKER_CMD="$DOCKER_CMD -v /shared/cookies:/shared/cookies"
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_URL=\"$SUPABASE_URL\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_PUBLISHABLE_KEY=\"$SUPABASE_PUBLISHABLE_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_SECRET_KEY=\"$SUPABASE_SECRET_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e BUILD_TIMESTAMP=\"$BUILD_TIMESTAMP\""
        
        # Add APP_DOMAIN (required for auth callbacks and cookie domain)
        [ -n "$APP_DOMAIN" ] && DOCKER_CMD="$DOCKER_CMD -e APP_DOMAIN=\"$APP_DOMAIN\""
        
        # Add optional environment variables if set
        [ -n "$RESEARCH_DATABASE_URL" ] && DOCKER_CMD="$DOCKER_CMD -e RESEARCH_DATABASE_URL=\"$RESEARCH_DATABASE_URL\""
        [ -n "$FMP_API_KEY" ] && DOCKER_CMD="$DOCKER_CMD -e FMP_API_KEY=\"$FMP_API_KEY\""
        
        # Note: WebAI cookies are now loaded from shared volume (/shared/cookies/webai_cookies.json)
        # The cookie refresher sidecar manages cookie refresh automatically
        # Initial cookies are set from WEBAI_COOKIES_JSON secret when sidecar starts
        
        # Add Ollama and Streamlit settings
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_BASE_URL=\"${OLLAMA_BASE_URL:-http://host.docker.internal:11434}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_MODEL=\"${OLLAMA_MODEL:-mistral-nemo:12b}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_ENABLED=\"${OLLAMA_ENABLED:-true}\""
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_SERVER_HEADLESS=true"
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_BROWSER_GATHER_USAGE_STATS=false"
        DOCKER_CMD="$DOCKER_CMD trading-dashboard:latest"
        
        # Execute the command
        eval $DOCKER_CMD
      
      # Deploy cookie refresher sidecar container (runs independently, persists across redeployments)
      - echo "Deploying cookie refresher sidecar..."
      - mkdir -p /shared/cookies && chmod 777 /shared/cookies
      # Build cookie refresher image (only rebuilds if Dockerfile changed)
      - docker build -f web_dashboard/Dockerfile.cookie-refresher -t cookie-refresher:latest .
      - docker tag cookie-refresher:latest cookie-refresher:${CI_COMMIT_SHA}
      # Only start container if it doesn't exist (idempotent - won't restart on redeploy)
      - |
        if ! docker ps -a --format '{{.Names}}' | grep -q '^cookie-refresher$'; then
          echo "Starting cookie refresher sidecar container..."
          # Copy initial cookies from Woodpecker secret if available
          if [ -n "$WEBAI_COOKIES_JSON" ]; then
            echo "$WEBAI_COOKIES_JSON" > /shared/cookies/webai_cookies.json
            echo "✅ Initialized cookie file from Woodpecker secret"
          fi
          # Start the sidecar container (persists across main app redeployments)
          # AI_SERVICE_WEB_URL is loaded from Woodpecker secret (obfuscated URL)
          docker run -d \
            --name cookie-refresher \
            --restart unless-stopped \
            -v /shared/cookies:/shared/cookies \
            -e COOKIE_REFRESH_INTERVAL=3600 \
            -e COOKIE_OUTPUT_FILE=/shared/cookies/webai_cookies.json \
            -e COOKIE_INPUT_FILE=/shared/cookies/webai_cookies.json \
            -e AI_SERVICE_WEB_URL="${AI_SERVICE_WEB_URL:-https://webai.google.com/app}" \
            cookie-refresher:latest
          echo "✅ Cookie refresher sidecar started"
        else
          echo "ℹ️  Cookie refresher sidecar already running (skipping start)"
        fi
      
      # Deploy static files (auth callback HTML, cookie setting page, and login form)
      - echo "Deploying static files..."
      - mkdir -p /deploy_target/frontend
      - cp web_dashboard/static/auth_callback.html /deploy_target/frontend/auth_callback.html
      - cp web_dashboard/static/set_cookie.html /deploy_target/frontend/set_cookie.html
      - cp web_dashboard/static/login.html /deploy_target/frontend/login.html
      
      # Deploy Research PDF files efficiently (only copy if Research folder exists and has PDFs)
      # Only copies files that are newer than destination (incremental update)
      - |
        if [ -d "Research" ] && [ "$(find Research -name '*.pdf' -type f 2>/dev/null | wc -l)" -gt 0 ]; then
          echo "Deploying Research PDF files..."
          mkdir -p /deploy_target/Research
          # Copy PDFs preserving directory structure, only if source is newer (efficient incremental)
          find Research -name '*.pdf' -type f | while read pdf_file; do
            dest_file="/deploy_target/$pdf_file"
            dest_dir=$(dirname "$dest_file")
            mkdir -p "$dest_dir"
            # Only copy if source is newer or destination doesn't exist
            if [ ! -f "$dest_file" ] || [ "$pdf_file" -nt "$dest_file" ]; then
              cp "$pdf_file" "$dest_file"
            fi
          done
          echo "✅ Research files deployed"
        else
          echo "ℹ️  No Research folder or PDFs found, skipping..."
        fi
      
      # Clean up old Docker images (keep last 5 versions)
      # This prevents disk space issues from accumulating old image versions
      - echo "Cleaning up old trading-dashboard images (keeping last 5)..."
      - docker images -q trading-dashboard | tail -n +6 | xargs -r docker rmi > /dev/null 2>&1 || true
      
      - echo "✅ Build and deployment complete!"
    when:
      event: push
      branch: [main, master]


