# Streamlit Dashboard Build and Deploy Pipeline
#
# This pipeline builds the Streamlit dashboard Docker image and deploys it to the server.
# It uses Docker socket mounting to build directly on the server (no SSH needed).
#
# Build Process:
# 1. Build Docker image from web_dashboard/Dockerfile
# 2. Tag image with commit SHA for version tracking
# 3. Stop and remove existing container (if running)
# 4. Deploy new container with environment variables from Woodpecker secrets
#
# Available Secrets (configure in Woodpecker - use lowercase names):
# - supabase_url: Your Supabase project URL
# - supabase_publishable_key: For user authentication
# - supabase_secret_key: For admin scripts and debug operations
# - app_domain: Your application domain (e.g., "ai-trading.drifting.space")
#               Used for magic links, password resets, and cookie domain settings
# - research_database_url: (optional) Postgres connection string for research articles
#                          Format: postgresql://user:password@host:port/database
#                          For Docker container connecting to host Postgres: use host.docker.internal
#                          Example: postgresql://postgres:password@host.docker.internal:5432/trading_db
# - fmp_api_key: (optional) Financial Modeling Prep API key for congress trading module
#                Get from: https://site.financialmodelingprep.com/developer/docs/
#                Required for congress_trades scheduled job
# - webai_cookies_json: (optional) JSON string of cookies for WebAI Pro web-based AI
#                       Format: {"__Secure-1PSID":"value","__Secure-1PSIDTS":"value"}
#                       Extract using: python web_dashboard/extract_ai_cookies.py --browser manual
#                       Required for WebAI Pro model in AI Assistant
#                       Note: Used only for initial cookie setup. A cookie refresher sidecar container
#                       automatically refreshes cookies and writes them to /shared/cookies/webai_cookies.json
#                       The main app reads cookies from the shared volume, not from this environment variable
# - ai_service_web_url: (optional) Web interface URL for AI service (obfuscated)
#                       Only needed if default fallback URL doesn't work
#                       Set via environment variable when generating ai_service.keys.json
# - ollama_base_url: (optional) Ollama API URL (default: http://host.docker.internal:11434)
# - ollama_model: (optional) Default AI model (default: mistral-nemo:12b)
# - ollama_enabled: (optional) Enable AI assistant (default: true)
#
# Note: Secrets are loaded using from_secret in the environment section
# Note: Repository must be marked as "Trusted" in Woodpecker settings to use volumes

steps:
  build-and-deploy:
    image: docker:24
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /home/lance/ai-trading-www:/deploy_target
    environment:
      TZ: America/Vancouver
      SUPABASE_URL:
        from_secret: supabase_url
      SUPABASE_PUBLISHABLE_KEY:
        from_secret: supabase_publishable_key
      SUPABASE_SECRET_KEY:
        from_secret: supabase_secret_key
      APP_DOMAIN:
        from_secret: app_domain
      RESEARCH_DATABASE_URL:
        from_secret: research_database_url
      FMP_API_KEY:
        from_secret: fmp_api_key
      WEBAI_COOKIES_JSON:
        from_secret: webai_cookies_json
      AI_SERVICE_WEB_URL:
        from_secret: ai_service_web_url
    commands:
      # Set timezone (Pacific Time - handles PST/PDT automatically)
      - ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
      
      # Format build timestamp in UTC (will be converted to user's timezone in Python)
      # CI_PIPELINE_STARTED is Unix timestamp, format in UTC
      - |
        if [ -n "$CI_PIPELINE_STARTED" ]; then
          export BUILD_TIMESTAMP=$(date -u -d "@$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u -r "$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u "+%Y-%m-%d %H:%M UTC")
        else
          export BUILD_TIMESTAMP=$(date -u "+%Y-%m-%d %H:%M UTC")
        fi
      
      # Build Docker image from web_dashboard/Dockerfile
      - docker build -f web_dashboard/Dockerfile -t trading-dashboard:latest .
      - docker tag trading-dashboard:latest trading-dashboard:${CI_COMMIT_SHA}
      
      # Stop and remove existing container if it exists
      - docker stop trading-dashboard || true
      - docker rm trading-dashboard || true
      
      # Create logs directory on host with proper permissions
      - mkdir -p /home/lance/trading-dashboard-logs && chmod 777 /home/lance/trading-dashboard-logs
      
      # Initialize shared cookies directory and file (before main container starts)
      - mkdir -p /shared/cookies && chmod 777 /shared/cookies
      - |
        if [ -n "$WEBAI_COOKIES_JSON" ]; then
          echo "$WEBAI_COOKIES_JSON" > /shared/cookies/webai_cookies.json
          chmod 644 /shared/cookies/webai_cookies.json
          echo "✅ Initialized cookie file from Woodpecker secret (before main container starts)"
          echo "   File location: /shared/cookies/webai_cookies.json"
          echo "   File size: $(wc -c < /shared/cookies/webai_cookies.json) bytes"
          echo "   File exists: $(test -f /shared/cookies/webai_cookies.json && echo 'YES' || echo 'NO')"
        else
          echo "⚠️  WARNING: WEBAI_COOKIES_JSON not set. Cookie file will be empty."
        fi
      
      # Verify secrets are set
      - |
        if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_PUBLISHABLE_KEY" ] || [ -z "$SUPABASE_SECRET_KEY" ]; then
          echo "❌ ERROR: Missing required secrets"
          echo "Make sure supabase_url, supabase_publishable_key, and supabase_secret_key are set in Woodpecker"
          exit 1
        fi
      
      # Deploy new container (single line to avoid parsing issues)
      # Added --add-host to allow connecting to Ollama on the host machine via host.docker.internal (Linux support)
      # NOTE: On Windows/Mac, host.docker.internal works natively. On Linux, you MUST add "--add-host=host.docker.internal:host-gateway"
      # Ollama settings can be overridden via Woodpecker secrets or environment variables
      # RESEARCH_DATABASE_URL is optional - only set if research articles storage is needed
      # FMP_API_KEY is optional - only set if congress trading module is needed
      - |
        # Build base docker run command
        DOCKER_CMD="docker run -d --name trading-dashboard --restart unless-stopped -p 8501:8501 --add-host=host.docker.internal:host-gateway"
        # Mount application logs directory (host logs -> container logs)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/trading-dashboard-logs:/app/web_dashboard/logs"
        # Mount Ollama logs directory (host logs -> container logs/server)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/ollama-logs:/app/web_dashboard/logs/server"
        # Mount shared cookies directory (for cookie refresher sidecar)
        DOCKER_CMD="$DOCKER_CMD -v /shared/cookies:/shared/cookies"
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_URL=\"$SUPABASE_URL\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_PUBLISHABLE_KEY=\"$SUPABASE_PUBLISHABLE_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_SECRET_KEY=\"$SUPABASE_SECRET_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e BUILD_TIMESTAMP=\"$BUILD_TIMESTAMP\""
        
        # Add APP_DOMAIN (required for auth callbacks and cookie domain)
        [ -n "$APP_DOMAIN" ] && DOCKER_CMD="$DOCKER_CMD -e APP_DOMAIN=\"$APP_DOMAIN\""
        
        # Add optional environment variables if set
        [ -n "$RESEARCH_DATABASE_URL" ] && DOCKER_CMD="$DOCKER_CMD -e RESEARCH_DATABASE_URL=\"$RESEARCH_DATABASE_URL\""
        [ -n "$FMP_API_KEY" ] && DOCKER_CMD="$DOCKER_CMD -e FMP_API_KEY=\"$FMP_API_KEY\""
        
        # Add WebAI cookies as environment variable (fallback if shared volume file not available)
        # The cookie refresher sidecar writes to /shared/cookies/webai_cookies.json
        # Main app checks shared volume first, then falls back to WEBAI_COOKIES_JSON_B64 env var
        # Use base64 encoding to avoid shell quoting issues with JSON
        if [ -n "$WEBAI_COOKIES_JSON" ]; then
          # Encode JSON as base64 to avoid shell quoting/escaping issues
          WEBAI_COOKIES_B64=$(echo -n "$WEBAI_COOKIES_JSON" | base64 -w 0)
          DOCKER_CMD="$DOCKER_CMD -e WEBAI_COOKIES_JSON_B64=$WEBAI_COOKIES_B64"
          echo "✅ WEBAI_COOKIES_JSON set (base64 encoded to avoid shell quoting issues)"
          echo "   Cookie file also written to /shared/cookies/webai_cookies.json"
          echo "   Base64 length: ${#WEBAI_COOKIES_B64} chars"
        else
          echo "⚠️  WARNING: WEBAI_COOKIES_JSON not set. WebAI Pro may not work until sidecar writes cookies."
        fi
        
        # Add Ollama and Streamlit settings
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_BASE_URL=\"${OLLAMA_BASE_URL:-http://host.docker.internal:11434}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_MODEL=\"${OLLAMA_MODEL:-mistral-nemo:12b}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_ENABLED=\"${OLLAMA_ENABLED:-true}\""
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_SERVER_HEADLESS=true"
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_BROWSER_GATHER_USAGE_STATS=false"
        DOCKER_CMD="$DOCKER_CMD trading-dashboard:latest"
        
        # Execute the command
        eval $DOCKER_CMD
        
        # Verify cookie file is accessible in container
        - |
          sleep 2  # Give container a moment to start
          if docker exec trading-dashboard test -f /shared/cookies/webai_cookies.json; then
            echo "✅ Cookie file is accessible in container"
            docker exec trading-dashboard head -c 100 /shared/cookies/webai_cookies.json
            echo "..."
          else
            echo "⚠️  Cookie file not found in container (will use base64 env var fallback)"
          fi
      
      # Deploy cookie refresher sidecar container (runs independently, persists across redeployments)
      - echo "Deploying cookie refresher sidecar..."
      - mkdir -p /shared/cookies && chmod 777 /shared/cookies
      # Build cookie refresher image (only rebuilds if Dockerfile changed)
      - docker build -f web_dashboard/Dockerfile.cookie-refresher -t cookie-refresher:latest .
      - docker tag cookie-refresher:latest cookie-refresher:${CI_COMMIT_SHA}
      # Only start container if it doesn't exist (idempotent - won't restart on redeploy)
      - |
        if ! docker ps -a --format '{{.Names}}' | grep -q '^cookie-refresher$'; then
          echo "Starting cookie refresher sidecar container..."
          # Copy initial cookies from Woodpecker secret if available
          if [ -n "$WEBAI_COOKIES_JSON" ]; then
            echo "$WEBAI_COOKIES_JSON" > /shared/cookies/webai_cookies.json
            echo "✅ Initialized cookie file from Woodpecker secret"
          fi
          # Start the sidecar container (persists across main app redeployments)
          # AI_SERVICE_WEB_URL is loaded from Woodpecker secret (obfuscated URL)
          docker run -d \
            --name cookie-refresher \
            --restart unless-stopped \
            -v /shared/cookies:/shared/cookies \
            -e COOKIE_REFRESH_INTERVAL=1800 \
            -e COOKIE_OUTPUT_FILE=/shared/cookies/webai_cookies.json \
            -e COOKIE_INPUT_FILE=/shared/cookies/webai_cookies.json \
            -e AI_SERVICE_WEB_URL="${AI_SERVICE_WEB_URL}" \
            cookie-refresher:latest
          echo "✅ Cookie refresher sidecar started"
        else
          echo "ℹ️  Cookie refresher sidecar already running (skipping start)"
        fi
      
      # Deploy static files (auth callback HTML, cookie setting page, and login form)
      - echo "Deploying static files..."
      - mkdir -p /deploy_target/frontend
      - cp web_dashboard/static/auth_callback.html /deploy_target/frontend/auth_callback.html
      - cp web_dashboard/static/set_cookie.html /deploy_target/frontend/set_cookie.html
      - cp web_dashboard/static/login.html /deploy_target/frontend/login.html
      
      # Deploy Research PDF files efficiently (only copy if Research folder exists and has PDFs)
      # Only copies files that are newer than destination (incremental update)
      - |
        if [ -d "Research" ] && [ "$(find Research -name '*.pdf' -type f 2>/dev/null | wc -l)" -gt 0 ]; then
          echo "Deploying Research PDF files..."
          mkdir -p /deploy_target/Research
          # Copy PDFs preserving directory structure, only if source is newer (efficient incremental)
          find Research -name '*.pdf' -type f | while read pdf_file; do
            dest_file="/deploy_target/$pdf_file"
            dest_dir=$(dirname "$dest_file")
            mkdir -p "$dest_dir"
            # Only copy if source is newer or destination doesn't exist
            if [ ! -f "$dest_file" ] || [ "$pdf_file" -nt "$dest_file" ]; then
              cp "$pdf_file" "$dest_file"
            fi
          done
          echo "✅ Research files deployed"
        else
          echo "ℹ️  No Research folder or PDFs found, skipping..."
        fi
      
      # Clean up old Docker images (keep last 5 versions)
      # This prevents disk space issues from accumulating old image versions
      - echo "Cleaning up old trading-dashboard images (keeping last 5)..."
      - docker images -q trading-dashboard | tail -n +6 | xargs -r docker rmi > /dev/null 2>&1 || true
      
      - echo "✅ Build and deployment complete!"
    when:
      event: push
      branch: [main, master]


